# Gradient-Based Optimization Algorithms - Animated  

This repository contains Jupyter Notebook code that generates animations for visualizing the convergence behavior of various gradient-based optimization algorithms, including:  

- **Gradient Descent (GD)**  
- **Adagrad**  
- **RMSprop**  
- **Adam**  

These animations were created for a presentation on *"Adam: A Method for Stochastic Optimization."*  

## ðŸ“Œ Features  

- Animated visualization of optimization algorithms on a loss surface.  
- Comparison of different methods in terms of convergence speed and path taken.  
- Interactive Jupyter Notebook (`gradient_based_optimization_algorithms_animated.ipynb`).  

## ðŸ“š Reference  

- Kingma, D. P., & Ba, J. (2015). *Adam: A Method for Stochastic Optimization.* In *International Conference on Learning Representations (ICLR)*. Retrieved from https://arxiv.org/abs/1412.6980
